{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x        float64\n",
      "y        float64\n",
      "z        float64\n",
      "label    float64\n",
      "dtype: object\n",
      "                  x             y             z         label\n",
      "count  10000.000000  10000.000000  10000.000000  10000.000000\n",
      "mean       0.850362     -3.108769     -2.601124      0.502700\n",
      "std      288.379928    287.120263    290.379789      0.500018\n",
      "min     -499.802348   -499.899134   -499.952571      0.000000\n",
      "25%     -249.199895   -248.954580   -258.005693      0.000000\n",
      "50%        3.663472     -5.446168     -8.221000      1.000000\n",
      "75%      248.879970    244.395864    252.930406      1.000000\n",
      "max      499.872453    499.752418    499.872329      1.000000\n"
     ]
    }
   ],
   "source": [
    "### pre-processing\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv('df_points.txt', sep = '\\t', usecols = ['x', 'y', 'z', 'label'])\n",
    "\n",
    "# check data dtypes\n",
    "print(df.dtypes)\n",
    "\n",
    "# check summary statistics\n",
    "print(df.describe())\n",
    "\n",
    "# shuffle samples\n",
    "df = shuffle(df)\n",
    "\n",
    "# separate attributes and label\n",
    "X, y = df[['x', 'y', 'z']], df['label']\n",
    "\n",
    "# standardize attributes: z = (x - u) / s\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# split test/train samples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "score obtained using C = 0.25 and solver = liblinear\n",
      "0.5124\n",
      " \n",
      "score obtained using C = 0.25 and solver = newton-cg\n",
      "0.512\n",
      " \n",
      "score obtained using C = 0.25 and solver = lbfgs\n",
      "0.512\n",
      " \n",
      "score obtained using C = 0.25 and solver = sag\n",
      "0.512\n",
      " \n",
      "score obtained using C = 0.25 and solver = saga\n",
      "0.512\n",
      " \n",
      "score obtained using C = 0.5 and solver = liblinear\n",
      "0.5124\n",
      " \n",
      "score obtained using C = 0.5 and solver = newton-cg\n",
      "0.5124\n",
      " \n",
      "score obtained using C = 0.5 and solver = lbfgs\n",
      "0.5124\n",
      " \n",
      "score obtained using C = 0.5 and solver = sag\n",
      "0.5124\n",
      " \n",
      "score obtained using C = 0.5 and solver = saga\n",
      "0.5124\n",
      " \n",
      "score obtained using C = 0.75 and solver = liblinear\n",
      "0.5124\n",
      " \n",
      "score obtained using C = 0.75 and solver = newton-cg\n",
      "0.5124\n",
      " \n",
      "score obtained using C = 0.75 and solver = lbfgs\n",
      "0.5124\n",
      " \n",
      "score obtained using C = 0.75 and solver = sag\n",
      "0.5124\n",
      " \n",
      "score obtained using C = 0.75 and solver = saga\n",
      "0.5124\n",
      " \n",
      "score obtained using C = 1.0 and solver = liblinear\n",
      "0.5124\n",
      " \n",
      "score obtained using C = 1.0 and solver = newton-cg\n",
      "0.5124\n",
      " \n",
      "score obtained using C = 1.0 and solver = lbfgs\n",
      "0.5124\n",
      " \n",
      "score obtained using C = 1.0 and solver = sag\n",
      "0.5124\n",
      " \n",
      "score obtained using C = 1.0 and solver = saga\n",
      "0.5124\n",
      " \n",
      "Logistic regression not good here: no better than tossing a coin.\n"
     ]
    }
   ],
   "source": [
    "### logistic regression\n",
    "\n",
    "# try different C values\n",
    "for C in [0.25, 0.5, 0.75, 1.0]:\n",
    "    \n",
    "    # try different solvers\n",
    "    for solver in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:\n",
    "\n",
    "        # initialize classifier\n",
    "        clf = LogisticRegression(C = C, solver = solver)\n",
    "\n",
    "        # train classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # check results\n",
    "        score = clf.score(X_test, y_test)\n",
    "        print(' ')\n",
    "        print('score obtained using C =', C, 'and solver =', solver)\n",
    "        print(score)\n",
    "\n",
    "print(' ')\n",
    "print('Logistic regression not good here: no better than tossing a coin.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "score obtained with GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "0.5928\n",
      "No good.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "score obtained with SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "0.7324\n",
      "We have a candidate algorithm.\n",
      " \n",
      "score obtained with RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.78\n",
      "We have a candidate algorithm.\n"
     ]
    }
   ],
   "source": [
    "### try other algorithms\n",
    "\n",
    "clf1 = GradientBoostingClassifier()\n",
    "clf2 = SVC()\n",
    "clf3 = RandomForestClassifier(n_estimators = 1000, min_samples_leaf = 10)\n",
    "\n",
    "for clf in [clf1, clf2, clf3]:\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print(' ')\n",
    "    print('score obtained with', clf)\n",
    "    print(score)\n",
    "    if score < 0.7:\n",
    "        print('No good.')\n",
    "    else:\n",
    "        print('We have a candidate algorithm.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "score obtained with k=1 and GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "0.63\n",
      "No good.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "score obtained with k=1 and SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "0.6444\n",
      "No good.\n",
      " \n",
      "score obtained with k=1 and RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.6036\n",
      "No good.\n"
     ]
    }
   ],
   "source": [
    "### what if we combine the three attributes into one?\n",
    "pca = PCA(n_components = 1)\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size = 0.25)\n",
    "\n",
    "clf1 = GradientBoostingClassifier()\n",
    "clf2 = SVC()\n",
    "clf3 = RandomForestClassifier(n_estimators = 1000, min_samples_leaf = 10)\n",
    "\n",
    "for clf in [clf1, clf2, clf3]:\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print(' ')\n",
    "    print('score obtained with k=1 and', clf)\n",
    "    print(score)\n",
    "    if score < 0.7:\n",
    "        print('No good.')\n",
    "    else:\n",
    "        print('We have a candidate algorithm.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 2/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 3/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 4/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 5/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 6/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 7/100\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 8/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 9/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 10/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 11/100\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 12/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 13/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 14/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 15/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 16/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 17/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 18/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 19/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 20/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 21/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 22/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 23/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 24/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 25/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 26/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 27/100\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 28/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 29/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 30/100\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 31/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 32/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 33/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 34/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 35/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 36/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 37/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 38/100\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 39/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 40/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 41/100\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 42/100\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 43/100\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 44/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 45/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 46/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 47/100\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 48/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 49/100\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 50/100\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 51/100\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 52/100\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 53/100\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 54/100\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 55/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 56/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 57/100\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 58/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 59/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 60/100\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 61/100\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 62/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 63/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 64/100\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 65/100\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 66/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 67/100\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 68/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 69/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 70/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 71/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 72/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 73/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 74/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 75/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 76/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 77/100\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 78/100\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 79/100\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 80/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 81/100\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 82/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 84/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 85/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 86/100\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 87/100\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 88/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 89/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 90/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 91/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 92/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 93/100\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 94/100\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 95/100\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 96/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 97/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 98/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 99/100\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.8968 - acc: 0.5047\n",
      "Epoch 100/100\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 7.8968 - acc: 0.5047\n",
      " \n",
      "score obtained with a simple neural network\n",
      "0.4968\n",
      "No good.\n"
     ]
    }
   ],
   "source": [
    "### try a neural network\n",
    "\n",
    "# go back to three attributes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "\n",
    "# build model\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim = 3, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'softmax'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size = 32, epochs = 100)\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print(' ')\n",
    "print('score obtained with a simple neural network')\n",
    "print(acc)\n",
    "if score < 0.7:\n",
    "    print('No good.')\n",
    "else:\n",
    "    print('We have a candidate algorithm.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "commentary on results\n",
      "\n",
      "Logistic regression did not work out: it classifies \n",
      "the samples correctly only ~50pct of the time, which\n",
      "is what we would get by simply tossing a coin. Since\n",
      "the samples are balanced (about 50pct belonging to each\n",
      "class), this is a terrible result.\n",
      "\n",
      "Gradient boosting and neural network yields a similar \n",
      "result. I.e., no good.\n",
      "\n",
      "Support vector machine yields a much better result: over\n",
      "70pct of correct classifications.\n",
      "\n",
      "Random forest yields the best result from the ones obtained\n",
      "in this exercise: almost 80pct of correct classifications.\n",
      "\n",
      "Combining the 3 attributes into 1 does not improve the results.\n",
      "(In fact it seems to worsen them.)\n",
      "\n",
      "The instruction sheet says \"Please do not spend too much \n",
      "time in it.\", so I am stopping here. In a real-world \n",
      "application there would be lots more to do, like using\n",
      "hyperparameter optimization to improve the random\n",
      "forest results, trying different neural network \n",
      "architectures, etc. Also, I would use cross validation instead\n",
      "of separating train/test samples only once (that would violate\n",
      "item (a) of the instruction sheet though).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "commentary = '''\n",
    "commentary on results\n",
    "\n",
    "Logistic regression did not work out: it classifies \n",
    "the samples correctly only ~50pct of the time, which\n",
    "is what we would get by simply tossing a coin. Since\n",
    "the samples are balanced (about 50pct belonging to each\n",
    "class), this is a terrible result.\n",
    "\n",
    "Gradient boosting and neural network yields a similar \n",
    "result. I.e., no good.\n",
    "\n",
    "Support vector machine yields a much better result: over\n",
    "70pct of correct classifications.\n",
    "\n",
    "Random forest yields the best result from the ones obtained\n",
    "in this exercise: almost 80pct of correct classifications.\n",
    "\n",
    "Combining the 3 attributes into 1 does not improve the results.\n",
    "(In fact it seems to worsen them.)\n",
    "\n",
    "The instruction sheet says \"Please do not spend too much \n",
    "time in it.\", so I am stopping here. In a real-world \n",
    "application there would be lots more to do, like using\n",
    "hyperparameter optimization to improve the random\n",
    "forest results, trying different neural network \n",
    "architectures, etc. Also, I would use cross validation instead\n",
    "of separating train/test samples only once (that would violate\n",
    "item (a) of the instruction sheet though).\n",
    "'''\n",
    "print(commentary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
